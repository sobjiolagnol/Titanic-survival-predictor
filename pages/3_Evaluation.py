import streamlit as st
from sklearn.utils import all_estimators
from utils import set_seed, load_csv, preprocess_data
import pandas as pd
import time
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import (
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
)

st.markdown(
    "<h2 style='text-align: center; color: #0366d6;'>üìù Evaluation</h2>",
    unsafe_allow_html=True,
)


st.subheader(
    (
        ":blue[Entra√Ænement]"
        if st.session_state.lang.startswith("fr")
        else ":blue[Training]"
    ),
    divider=True,
)

set_seed()

# R√©cup√©rer tous les classifiers
all_classifiers = all_estimators(type_filter="classifier")

st.write(
    "Les diff√©rents mod√®les de Machine Learning de la librairie Scikit-learn sont entra√Æn√©s avec leurs param√®tres par d√©faut puis class√©s selon 3 scoring diff√©rents (balanced accuracy, ROC AUC et f1-score). L'√©valuation est r√©alis√©e par Cross Validation √† 5 folds sur un ensemble d‚Äôentra√Ænement constitu√© de 80% des donn√©es disponibles."
    if st.session_state.lang.startswith("fr")
    else "The various Machine Learning models from the Scikit-learn library are trained with their default parameters, then ranked based on three different scoring metrics (balanced accuracy, ROC AUC, and F1-score). Their evaluation is computed using 5-fold cross-validation on a training set composed of 80% of the available data."
)

df = load_csv(drop_outliers=True)

X_train, X_test, y_train, y_test = preprocess_data(df, split=True)


# R√©cup√©rer tous les classifiers
all_classifiers = all_estimators(type_filter="classifier")


results = []
errors = []
df_results = pd.DataFrame()

progress_bar = st.progress(0)
status = st.empty()
results_placeholder = st.empty()
spinner_placeholder = st.empty()

container = st.container()

total = len(all_classifiers)


start_total_time = time.time()

skf = StratifiedKFold(n_splits=5, shuffle=True)

for i, (name, ClfClass) in enumerate(all_classifiers):

    with spinner_placeholder:
        with st.spinner(f"Training {name}", show_time=True):

            progress_bar.progress((i + 1) / total)
            status.text(f"{i+1}/{total} - {name}")

            try:
                clf = ClfClass()
                start_time = time.time()

                bal_acc_scores = cross_val_score(
                    clf, X_train, y_train, cv=skf, scoring="balanced_accuracy"
                )

                roc_auc_scores = f1_scores = cross_val_score(
                    clf, X_train, y_train, cv=skf, scoring="roc_auc"
                )

                f1_scores = cross_val_score(clf, X_train, y_train, cv=skf, scoring="f1")

                bal_acc_mean = bal_acc_scores.mean()
                roc_auc_mean = roc_auc_scores.mean()
                f1_mean = f1_scores.mean()

                end_time = time.time()
                duration = int((end_time - start_time) * 1000)

                if pd.isna(bal_acc_mean) or pd.isna(roc_auc_mean) or pd.isna(f1_mean):
                    raise ValueError("Scores invalides (nan)")

                results.append(
                    {
                        "Model": name,
                        "Balanced Accuracy (%)": round(100 * bal_acc_mean, 2),
                        "ROC AUC": roc_auc_mean,
                        "f1-score": f1_mean,
                        "Time (ms)": duration,
                    }
                )
            except Exception as e:
                errors.append({"Model": name, "Error": e})

            # Afficher sous forme de DataFrame tri√©e par Accuracy d√©croissante
            df_results = pd.DataFrame(results)
            df_results = df_results.sort_values(
                by="Balanced Accuracy (%)", ascending=False
            ).reset_index(drop=True)

            results_placeholder.dataframe(df_results)

duration = round(time.time() - start_total_time, 1)

status.text("")

container.success(
    f"{len(results)} "
    + (
        "mod√®les ont √©t√© √©valu√©s en"
        if st.session_state.lang.startswith("fr")
        else "models were evaluated in"
    )
    + f" {duration} s",
    icon="‚úÖ",
)

container.warning(
    f"{len(errors)} "
    + (
        "mod√®les n'ont pas pu √™tre entra√Æn√©s"
        if st.session_state.lang.startswith("fr")
        else "models could not be trained"
    ),
    icon="‚ÑπÔ∏è",
)

st.caption(
    (
        "seed de la session = "
        if st.session_state.lang.startswith("fr")
        else "session seed = "
    )
    + f"{st.session_state.seed}"
)

with st.expander(
    "Afficher les erreurs"
    if st.session_state.lang.startswith("fr")
    else "Display errors"
):
    st.dataframe(errors)


best_model_name = df_results.iloc[0, 0]

st.subheader(":blue[Evaluation]", divider=True)

st.write(f"üèÜ {best_model_name}")

st.write(
    f"Le mod√®le ayant obtenu les meilleures performances durant l'entra√Ænement (phase d'ajustement des param√®tres) est {best_model_name}. Son √©valuation finale est r√©alis√©e sur un ensemble de test constitu√© des 20 % de donn√©es non utilis√©es lors de l'ajustement du mod√®le (hold-out)."
    if st.session_state.lang.startswith("fr")
    else f"The best-performing model during training (parameters fitting phase) was {best_model_name}. Its performance is evaluated on a hold-out test set comprising 20% of the data that was not used during model fitting."
)

for name, Clf in all_classifiers:
    if name == best_model_name:
        best_model = Clf()
        break
else:
    raise ValueError(f"Impossible de trouver {best_model_name} dans all_classifiers")

assert best_model is not None, f"best_model_name {best_model_name} non trouv√©"

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

assert y_test is not None
# y_test is not none with preprocess_data(df, split=True)

balanced_acc = round(100 * balanced_accuracy_score(y_test, y_pred), 2)
st.write(f"- Balanced accuracy = **{balanced_acc} %**")


# Afficher classification_report sous forme de DataFrame
report_dict = classification_report(y_test, y_pred, output_dict=True)
df_report = pd.DataFrame(report_dict).transpose()
st.write("- Classification Report")
st.dataframe(df_report)

# Afficher la matrice de confusion
cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=["Actual 0", "Actual 1"], columns=["Pred 0", "Pred 1"])
st.write("- Confusion Matrix")
st.dataframe(df_cm)


_, col, _ = st.columns(3)
with col:
    st.write("")
    st.write("")
    st.page_link(
        st.Page(
            "pages/4_Optimisation.py",
            title=(
                "Passer √† l'√©tape suivante"
                if st.session_state.lang.startswith("fr")
                else "Go to the next step"
            ),
            icon="‚û°Ô∏è",
        )
    )
st.divider()
st.markdown(
    """
    <div style='text-align: center; font-size: small; color: gray;'>
    ¬© 2025 Lagnol SOBJIO
    </div>
    """,
    unsafe_allow_html=True,
)
